#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒ»ç™‚æƒ…å ±RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãƒ»è¦ç´„ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆè‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³å¯¾å¿œç‰ˆï¼‰
Safariã§IDã¨PWã‚’å…¥ã‚Œã¦ã‚µã‚¤ãƒˆã«è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã—ã€
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰åŒ»ç™‚æƒ…å ±ã‚’å–å¾—ã—ã¦Geminiã§è¦ç´„ã™ã‚‹
"""

from string import Template
import requests
import feedparser
from bs4 import BeautifulSoup
import datetime
from pathlib import Path
import sys
import os
from dotenv import load_dotenv
from typing import List, Dict, Optional
import time
import subprocess
import re
import json
import pickle

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
env_path = Path(__file__).parent.parent.parent / 'env.local'
load_dotenv(dotenv_path=env_path)

# Geminiãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
import os
# ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
current_file = Path(__file__).resolve()
# modulesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—
modules_path = current_file.parent.parent / 'daily_report' / 'modules'
# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(modules_path))
# ç›´æ¥geminiãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import gemini
import teams
import teams_notification


class MedifaxAutoLogin:
    """åŒ»ç™‚æƒ…å ±RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãƒ»è¦ç´„ã‚¯ãƒ©ã‚¹ï¼ˆè‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³å¯¾å¿œï¼‰"""

    def __init__(self):
        self.rss_url = "https://mfd.jiho.jp/genre/1/rss.xml"
        self.login_url = "https://mfd.jiho.jp/login"  # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®URL
        self.output_dir = Path(current_file.parent.parent.parent / "80-MEDIFAX_DIGEST")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        self.username = os.getenv("MEDIFAX_USERNAME")
        self.password = os.getenv("MEDIFAX_PASSWORD")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        # ãƒ­ã‚°ã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«
        self.cache_dir = Path(current_file.parent / ".cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.login_cache_file = self.cache_dir / "medifax_login.pkl"
        self.cookie_cache_file = self.cache_dir / "medifax_cookies.pkl"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å¾©å…ƒ
        self._load_cached_session()

    def _load_cached_session(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.login_cache_file.exists():
                # ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã®æœ‰åŠ¹æœŸé™ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ24æ™‚é–“ï¼‰
                cache_time = datetime.datetime.fromtimestamp(self.login_cache_file.stat().st_mtime)
                if datetime.datetime.now() - cache_time < datetime.timedelta(hours=24):
                    with open(self.login_cache_file, 'rb') as f:
                        login_info = pickle.load(f)
                        print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™")
                        self.cached_login = True

                        # ã‚¯ãƒƒã‚­ãƒ¼æƒ…å ±ã‚‚å¾©å…ƒ
                        if self.cookie_cache_file.exists():
                            with open(self.cookie_cache_file, 'rb') as f:
                                cookies = pickle.load(f)
                                self.session.cookies.update(cookies)
                        return
                else:
                    print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ãŒæœŸé™åˆ‡ã‚Œã§ã™")

            self.cached_login = False

        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.cached_login = False

    def _save_cached_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã®è¨˜éŒ²
            with open(self.login_cache_file, 'wb') as f:
                pickle.dump({'logged_in': True, 'timestamp': datetime.datetime.now()}, f)

            # ã‚¯ãƒƒã‚­ãƒ¼æƒ…å ±ã‚’ä¿å­˜
            with open(self.cookie_cache_file, 'wb') as f:
                pickle.dump(dict(self.session.cookies), f)

            print("ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            if self.login_cache_file.exists():
                self.login_cache_file.unlink()
            if self.cookie_cache_file.exists():
                self.cookie_cache_file.unlink()
            print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

    def setup_safari_automation(self):
        """Safariã§ã®è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³è¨­å®š"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if hasattr(self, 'cached_login') and self.cached_login:
            print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ä¸­ - Safariãƒ­ã‚°ã‚¤ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return True

        print("Safariã§ã®è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³è¨­å®šã‚’ç¢ºèªä¸­...")

        # AppleScriptã§Safariã‚’é–‹ã„ã¦ãƒ­ã‚°ã‚¤ãƒ³
        if self.username and self.password:
            print("ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return self.login_with_safari()
        else:
            print("ç’°å¢ƒå¤‰æ•° MEDIFAX_USERNAME ã¨ MEDIFAX_PASSWORD ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("æ‰‹å‹•ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
            return self.manual_login_prompt()

    def login_with_safari(self):
        """Safariã§è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆAppleScriptã«ã‚ˆã‚‹DOMæ“ä½œï¼‰"""
        import subprocess

        def escape_quotes(s):
            return s.replace('"', '\\"') if s else ""

        login_url = escape_quotes(self.login_url)

        # Safariã§æ–°ã—ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‹ãAppleScriptï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„ï¼‰
        apple_script = f'''
        tell application "Safari"
            activate
            delay 1
            -- æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
            set loginDoc to make new document
            delay 1
            -- URLã‚’è¨­å®šï¼ˆdocumentã«å¯¾ã—ã¦ç›´æ¥è¨­å®šï¼‰
            set URL of loginDoc to "{login_url}"
            -- ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
            delay 6
        end tell
        
        -- ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›
        tell application "System Events"
            tell process "Safari"
                set frontmost to true
                delay 1
                -- è‹±æ•°å…¥åŠ›ã«åˆ‡ã‚Šæ›¿ãˆ
                key code 102
                delay 0.5
                -- ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›
                keystroke "{self.username}"
                delay 0.5
                -- Tabã§ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ç§»å‹•
                keystroke tab
                delay 0.5
                -- è‹±æ•°å…¥åŠ›ã‚’å†ç¢ºèª
                key code 102
                delay 0.5
                -- ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
                keystroke "{self.password}"
                delay 0.5
                -- Enterã§ãƒ­ã‚°ã‚¤ãƒ³
                keystroke return
            end tell
        end tell
        delay 3
        '''

        print("Safariã§è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚’å®Ÿè¡Œä¸­...")

        try:
            result = subprocess.run(
                ['osascript', '-e', apple_script],
                capture_output=True, text=True
            )
            if result.returncode == 0:
                print("Safariã§ã®è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸ")
                # ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—
                success = self.get_safari_session()
                if success:
                    # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸæ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
                    self._save_cached_session()
                return success
            else:
                print(f"Safariè‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return self.manual_login_prompt()
        except Exception as e:
            print(f"Safariè‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return self.manual_login_prompt()

    def get_safari_session(self):
        """Safariã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã—ã¦requestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã«åæ˜ """
        try:
            print("Safariã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ä¸­...")

            # Safariã®ã‚¯ãƒƒã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            # (ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦indexã¯ä½¿ã‚ãªã„ãŒã€ä»Šå¾Œæ‹¡å¼µã™ã‚‹å ´åˆã¯åˆ©ç”¨å¯)
            apple_script = '''
            tell application "Safari"
                set cookieData to ""
                if (count of windows) > 0 then
                    repeat with t in tabs of front window
                        set currentURL to URL of t
                        if currentURL contains "mfd.jiho.jp" then
                            set cookieData to "found"
                            exit repeat
                        end if
                    end repeat
                end if
                return cookieData
            end tell
            '''

            result = subprocess.run(
                ['osascript', '-e', apple_script],
                capture_output=True, text=True
            )

            if result.returncode == 0 and "found" in result.stdout:
                print("Safariã§ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¾ã—ãŸ")
                # Safariã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¨¡å€£ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
                self.session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',
                    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                    'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1'
                })
                return True
            else:
                print("Safariã§ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return self.manual_login_prompt()

        except Exception as e:
            print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return self.manual_login_prompt()

    def fetch_rss_with_safari(self) -> List[Dict]:
        """Safariã§RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            print("Safariã§RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­...")

            # Safariã§RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’é–‹ãï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
            apple_script = f'''
            tell application "Safari"
                activate
                delay 1
                -- æ—¢å­˜ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ
                if (count of windows) > 0 then
                    set rssDoc to front document
                else
                    set rssDoc to make new document
                end if
                delay 0.5
                set URL of rssDoc to "{self.rss_url}"
                -- ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
                delay 5
                -- ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
                set rssContent to source of rssDoc
                return rssContent
            end tell
            '''

            result = subprocess.run(
                ['osascript', '-e', apple_script],
                capture_output=True, text=True
            )

            if result.returncode == 0 and result.stdout.strip():
                print("Safariã‹ã‚‰RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸ")
                xml_content = result.stdout.strip()

                # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
                feed = feedparser.parse(xml_content)

                print(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ {len(feed.entries)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—")

                # è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
                articles = []
                today = datetime.date.today()
                print(f"\nä»Šæ—¥ã®æ—¥ä»˜: {today}")

                for i, entry in enumerate(feed.entries):
                    print(f"\nè¨˜äº‹ {i+1}: {entry.get('title', 'No title')}")

                    # å„ç¨®æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å–å¾—
                    dc_date = entry.get('dc_date', '')
                    published = entry.get('published', '')
                    published_parsed = entry.get('published_parsed', None)
                    updated = entry.get('updated', '')
                    updated_parsed = entry.get('updated_parsed', None)
                    jdate = getattr(entry, 'dc_jdate', '') if hasattr(entry, 'dc_jdate') else ''

                    print(f"  dc_date: {dc_date}")
                    print(f"  published: {published}")
                    print(f"  updated: {updated}")
                    print(f"  jdate: {jdate}")

                    # æ—¥ä»˜åˆ¤å®š
                    is_today = False
                    # å„ªå…ˆé †ä½: dc_date > published > updated > jdate
                    date_str = None
                    if dc_date:
                        date_str = dc_date
                    elif published:
                        date_str = published
                    elif updated:
                        date_str = updated
                    elif jdate:
                        date_str = jdate

                    # ISO8601å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                    entry_date = None
                    if date_str:
                        print(f"  å‡¦ç†ã™ã‚‹æ—¥ä»˜æ–‡å­—åˆ—: {date_str}")
                        try:
                            dt = datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                            entry_date = dt.date()
                            print(f"  ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸæ—¥ä»˜: {entry_date}")
                        except Exception as e:
                            print(f"  ISO8601ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                            m = re.match(r"(\d{4})-(\d{2})-(\d{2})", date_str)
                            if m:
                                entry_date = datetime.date(int(m.group(1)), int(m.group(2)), int(m.group(3)))
                                print(f"  æ­£è¦è¡¨ç¾ã§ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸæ—¥ä»˜: {entry_date}")

                    # published_parsedã¾ãŸã¯updated_parsedãŒã‚ã‚Œã°ãã¡ã‚‰ã‚‚è¦‹ã‚‹
                    if not entry_date and published_parsed:
                        entry_date = datetime.date(published_parsed.tm_year, published_parsed.tm_mon, published_parsed.tm_mday)
                        print(f"  published_parsedã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜: {entry_date}")
                    elif not entry_date and updated_parsed:
                        entry_date = datetime.date(updated_parsed.tm_year, updated_parsed.tm_mon, updated_parsed.tm_mday)
                        print(f"  updated_parsedã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜: {entry_date}")

                    if entry_date and entry_date == today:
                        is_today = True
                        print(f"  âœ“ ä»Šæ—¥ã®è¨˜äº‹ã¨ã—ã¦åˆ¤å®š")
                    else:
                        print(f"  âœ— ä»Šæ—¥ã®è¨˜äº‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (entry_date: {entry_date}, today: {today})")

                    if not is_today:
                        continue

                    article = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'published': published,
                        'dc_date': dc_date,
                        'jdate': jdate,
                        'content': ''
                    }
                    articles.append(article)
                    print(f"  âœ“ è¨˜äº‹ã‚’è¿½åŠ ")

                print(f"\n{len(articles)}ä»¶ã®æœ¬æ—¥åˆ†ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return articles

            else:
                print("Safariã‹ã‚‰RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return []

        except Exception as e:
            print(f"Safari RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def manual_login_prompt(self):
        """æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ã®æ¡ˆå†…"""
        print("\n=== æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ ===")
        print(f"1. Safariã§ {self.login_url} ã‚’é–‹ã„ã¦ãã ã•ã„")
        print("2. ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
        print("3. ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
        print("   â€» æ—¢å­˜ã®Safariã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½¿ç”¨ã—ã¾ã™")

        input("\nãƒ­ã‚°ã‚¤ãƒ³ãŒå®Œäº†ã—ãŸã‚‰ Enter ã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
        # æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³å¾Œã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
        self._save_cached_session()
        return True

    def fetch_rss_feed(self) -> List[Dict]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦æœ¬æ—¥åˆ†ã®è¨˜äº‹æƒ…å ±ã®ã¿æŠ½å‡º"""
        try:
            print(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­: {self.rss_url}")

            # ã¾ãšrequestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã§è©¦è¡Œ
            response = self.session.get(self.rss_url, timeout=30)
            response.raise_for_status()

            print(f"RSSãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
            print(f"RSSãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.content)} bytes")

            # ç”Ÿã®XMLå†…å®¹ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            xml_content = response.content.decode('utf-8', errors='ignore')
            print(f"\n=== RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®æœ€åˆã®1000æ–‡å­— ===")
            print(xml_content[:1000])
            print("=== RSSãƒ•ã‚£ãƒ¼ãƒ‰å†…å®¹çµ‚äº† ===\n")

            # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå ´åˆã®ãƒã‚§ãƒƒã‚¯
            if "ãƒ­ã‚°ã‚¤ãƒ³" in xml_content or "login" in xml_content.lower() or len(xml_content) < 1000:
                print("ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªã‚ˆã†ã§ã™ã€‚Safariã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œã—ã¾ã™...")
                return self.fetch_rss_with_safari()

            # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
            feed = feedparser.parse(response.content)

            print(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ {len(feed.entries)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—")

            # æœ€åˆã®3ä»¶ã®è¨˜äº‹ã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            if feed.entries:
                print("\n=== æœ€åˆã®3ä»¶ã®è¨˜äº‹ã®è©³ç´° ===")
                for i, entry in enumerate(feed.entries[:3]):
                    print(f"\nè¨˜äº‹ {i+1}:")
                    print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {entry.get('title', 'No title')}")
                    print(f"  ãƒªãƒ³ã‚¯: {entry.get('link', 'No link')}")
                    print(f"  dc_date: {entry.get('dc_date', 'No dc_date')}")
                    print(f"  published: {entry.get('published', 'No published')}")
                    print(f"  published_parsed: {entry.get('published_parsed', 'No published_parsed')}")
                    if hasattr(entry, 'dc_jdate'):
                        print(f"  dc_jdate: {entry.dc_jdate}")
                    print(f"  å…¨ã‚­ãƒ¼: {list(entry.keys())}")

            articles = []
            today = datetime.date.today()
            print(f"\nä»Šæ—¥ã®æ—¥ä»˜: {today}")

            for i, entry in enumerate(feed.entries):
                print(f"\nè¨˜äº‹ {i+1}: {entry.get('title', 'No title')}")

                # å„ç¨®æ—¥ä»˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å–å¾—
                dc_date = entry.get('dc_date', '')
                published = entry.get('published', '')
                published_parsed = entry.get('published_parsed', None)
                updated = entry.get('updated', '')
                updated_parsed = entry.get('updated_parsed', None)
                jdate = getattr(entry, 'dc_jdate', '') if hasattr(entry, 'dc_jdate') else ''

                # feedparserã§ã®åå‰ç©ºé–“ä»˜ãè¦ç´ ã®ç¢ºèª
                print(f"  dc_date: {dc_date}")
                print(f"  published: {published}")
                print(f"  updated: {updated}")
                print(f"  jdate: {jdate}")

                # åå‰ç©ºé–“ä»˜ãè¦ç´ ã®ç›´æ¥ç¢ºèª
                if hasattr(entry, 'dc_date'):
                    print(f"  entry.dc_date: {entry.dc_date}")
                if hasattr(entry, 'dc_jdate'):
                    print(f"  entry.dc_jdate: {entry.dc_jdate}")

                # å…¨å±æ€§ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                print(f"  entry.__dict__.keys(): {list(entry.__dict__.keys())}")

                # æ—¥ä»˜åˆ¤å®š
                is_today = False
                # å„ªå…ˆé †ä½: dc_date > published > updated > jdate
                date_str = None
                if dc_date:
                    date_str = dc_date
                elif published:
                    date_str = published
                elif updated:
                    date_str = updated
                elif jdate:
                    date_str = jdate

                # ISO8601å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                entry_date = None
                if date_str:
                    print(f"  å‡¦ç†ã™ã‚‹æ—¥ä»˜æ–‡å­—åˆ—: {date_str}")
                    # ä¾‹: 2025-08-01T05:00:05+09:00
                    try:
                        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãã®å ´åˆ
                        dt = datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        entry_date = dt.date()
                        print(f"  ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸæ—¥ä»˜: {entry_date}")
                    except Exception as e:
                        print(f"  ISO8601ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                        # ãã‚Œä»¥å¤–ã®å½¢å¼ã®å ´åˆ
                        m = re.match(r"(\d{4})-(\d{2})-(\d{2})", date_str)
                        if m:
                            entry_date = datetime.date(int(m.group(1)), int(m.group(2)), int(m.group(3)))
                            print(f"  æ­£è¦è¡¨ç¾ã§ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸæ—¥ä»˜: {entry_date}")

                # published_parsedã¾ãŸã¯updated_parsedãŒã‚ã‚Œã°ãã¡ã‚‰ã‚‚è¦‹ã‚‹
                if not entry_date and published_parsed:
                    entry_date = datetime.date(published_parsed.tm_year, published_parsed.tm_mon, published_parsed.tm_mday)
                    print(f"  published_parsedã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜: {entry_date}")
                elif not entry_date and updated_parsed:
                    entry_date = datetime.date(updated_parsed.tm_year, updated_parsed.tm_mon, updated_parsed.tm_mday)
                    print(f"  updated_parsedã‹ã‚‰å–å¾—ã—ãŸæ—¥ä»˜: {entry_date}")

                if entry_date and entry_date == today:
                    is_today = True
                    print(f"  âœ“ ä»Šæ—¥ã®è¨˜äº‹ã¨ã—ã¦åˆ¤å®š")
                else:
                    print(f"  âœ— ä»Šæ—¥ã®è¨˜äº‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (entry_date: {entry_date}, today: {today})")

                if not is_today:
                    continue

                article = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'published': published,
                    'dc:date': dc_date,
                    'jdate': jdate,
                    'content': ''
                }
                articles.append(article)
                print(f"  âœ“ è¨˜äº‹ã‚’è¿½åŠ ")

            print(f"\n{len(articles)}ä»¶ã®æœ¬æ—¥åˆ†ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return articles

        except Exception as e:
            print(f"RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def fetch_article_content(self, url: str) -> str:
        """è¨˜äº‹URLã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—ï¼ˆãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½¿ç”¨ï¼‰"""
        try:
            print(f"è¨˜äº‹å†…å®¹ã‚’å–å¾—ä¸­: {url}")

            # ã¾ãšrequestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã§è©¦è¡Œ
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ï¼‰
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '#content',
                '.content',
                '.article-body',
                '.post-body'
            ]

            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(strip=True)
                    break

            # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
            if not content:
                # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
                for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                    tag.decompose()
                content = soup.get_text(strip=True)

            # ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå ´åˆã®ãƒã‚§ãƒƒã‚¯
            if len(content) < 500 or "ãƒ­ã‚°ã‚¤ãƒ³" in content or "login" in content.lower():
                print("ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªã‚ˆã†ã§ã™ã€‚Safariã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’è©¦è¡Œã—ã¾ã™...")
                return self.fetch_article_content_with_safari(url)

            return content[:5000]  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚

        except Exception as e:
            print(f"è¨˜äº‹å†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return self.fetch_article_content_with_safari(url)

    def fetch_article_content_with_safari(self, url: str) -> str:
        """Safariã§è¨˜äº‹å†…å®¹ã‚’å–å¾—ï¼ˆãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¶­æŒï¼‰"""
        try:
            print(f"Safariã§è¨˜äº‹å†…å®¹ã‚’å–å¾—ä¸­: {url}")
            
            # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
            escaped_url = url.replace('"', '\\"')

            # Safariã§è¨˜äº‹ã‚’é–‹ãï¼ˆãƒ­ã‚°ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¶­æŒã—ãŸã¾ã¾ï¼‰
            apple_script = f'''
            tell application "Safari"
                activate
                
                -- ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦/ã‚¿ãƒ–ã‚’æ¢ã™
                set foundLoggedIn to false
                set targetWindow to missing value
                
                repeat with w in windows
                    repeat with t in tabs of w
                        try
                            set tabURL to URL of t
                            if tabURL contains "mfd.jiho.jp" then
                                set foundLoggedIn to true
                                set targetWindow to w
                                exit repeat
                            end if
                        end try
                    end repeat
                    if foundLoggedIn then exit repeat
                end repeat
                
                -- ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒã‚ã‚Œã°ãã“ã§é–‹ãã€ãªã‘ã‚Œã°æ–°è¦
                if foundLoggedIn and targetWindow is not missing value then
                    set current tab of targetWindow to make new tab at targetWindow
                    set URL of current tab of targetWindow to "{escaped_url}"
                else
                    -- æ–°è¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§é–‹ã
                    set newDoc to make new document
                    set URL of newDoc to "{escaped_url}"
                end if
                
                -- ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã‚’é•·ã‚ã«å¾…ã¤ï¼ˆèªè¨¼ã‚„ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆï¼‰
                delay 8
                
                -- ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
                set articleContent to source of front document
                return articleContent
            end tell
            '''

            result = subprocess.run(
                ['osascript', '-e', apple_script],
                capture_output=True, text=True
            )

            if result.returncode == 0 and result.stdout.strip():
                html_content = result.stdout.strip()
                
                # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if "ãƒ­ã‚°ã‚¤ãƒ³" in html_content[:500] or "login" in html_content[:500].lower():
                    print("ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚")
                    return self.fetch_article_with_manual_safari(url)
                
                print("Safariã‹ã‚‰è¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¾ã—ãŸ")
                soup = BeautifulSoup(html_content, 'html.parser')

                # MEDIFAXã®è¨˜äº‹æ§‹é€ ã«ç‰¹åŒ–ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿
                content_selectors = [
                    '.article-detail',  # MEDIFAXç‰¹æœ‰
                    '.article-body',
                    '.article-content', 
                    '.main-content',
                    'article',
                    '.post-content',
                    '.entry-content',
                    'main',
                    '#content',
                    '.content'
                ]

                content = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)
                        if len(content) > 100:  # æœ‰åŠ¹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ç¢ºèª
                            break

                # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
                if not content or len(content) < 100:
                    # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
                    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu', 'button']):
                        tag.decompose()
                    content = soup.get_text(strip=True)

                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã‚‹å ´åˆã¯æ‰‹å‹•å–å¾—ã‚’ä¿ƒã™
                if len(content) < 200:
                    print(f"å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(content)}æ–‡å­—ï¼‰ã€‚æ‰‹å‹•å–å¾—ã‚’è©¦ã¿ã¾ã™ã€‚")
                    return self.fetch_article_with_manual_safari(url)

                return content[:5000]  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚

            else:
                print("Safariã‹ã‚‰è¨˜äº‹å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return self.fetch_article_with_manual_safari(url)

        except Exception as e:
            print(f"Safariè¨˜äº‹å†…å®¹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return self.fetch_article_with_manual_safari(url)
    
    def fetch_article_with_manual_safari(self, url: str) -> str:
        """Safariã§è¨˜äº‹ã‚’é–‹ã„ã¦æ‰‹å‹•ã§å†…å®¹ã‚’ç¢ºèªã—ã¦ã‚‚ã‚‰ã†"""
        try:
            print(f"\n=== æ‰‹å‹•ã§ã®è¨˜äº‹ç¢ºèªãŒå¿…è¦ã§ã™ ===")
            print(f"URL: {url}")
            
            # Safariã§è¨˜äº‹ã‚’é–‹ã
            escaped_url = url.replace('"', '\\"')
            apple_script = f'''
            tell application "Safari"
                activate
                open location "{escaped_url}"
            end tell
            '''
            
            subprocess.run(['osascript', '-e', apple_script])
            
            print("\n1. Safariã§è¨˜äº‹ãŒé–‹ãã¾ã—ãŸ")
            print("2. å¿…è¦ã«å¿œã˜ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„")
            print("3. è¨˜äº‹ãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„")
            print("   â€» è¨˜äº‹ã®å†…å®¹ã‚’è‡ªå‹•çš„ã«å–å¾—ã—ã¾ã™\n")
            
            input("æº–å‚™ãŒã§ããŸã‚‰Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
            
            # ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã®ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            apple_script_get = '''
            tell application "Safari"
                set pageSource to source of front document
                return pageSource
            end tell
            '''
            
            result = subprocess.run(
                ['osascript', '-e', apple_script_get],
                capture_output=True, text=True
            )
            
            if result.returncode == 0 and result.stdout.strip():
                html_content = result.stdout.strip()
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
                content_selectors = [
                    '.article-detail',
                    '.article-body',
                    '.article-content',
                    '.main-content',
                    'article',
                    'main',
                    '.content'
                ]
                
                content = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)
                        if len(content) > 100:
                            break
                
                if not content or len(content) < 100:
                    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu', 'button']):
                        tag.decompose()
                    content = soup.get_text(strip=True)
                
                if content and len(content) > 100:
                    print(f"è¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ{len(content)}æ–‡å­—ï¼‰")
                    return content[:5000]
                else:
                    print("è¨˜äº‹å†…å®¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return "è¨˜äº‹å†…å®¹ã‚’è‡ªå‹•å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªãŒå¿…è¦ã§ã™ã€‚"
            
            return "è¨˜äº‹å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            print(f"æ‰‹å‹•å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return f"è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"

    def summarize_article(self, title: str, content: str) -> str:
        """Geminiã§è¨˜äº‹ã‚’è¦ç´„"""
        try:
            prompt = f"""
ä»¥ä¸‹ã®åŒ»ç™‚æƒ…å ±è¨˜äº‹ã‚’èª­ã¿ã‚„ã™ãæ•´ç†ã—ã€è©³ç´°ãªè§£èª¬ã¨è¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}

è¨˜äº‹å†…å®¹:
{content}

å‡ºåŠ›å½¢å¼:
ã€æ¦‚è¦ã€‘
è¨˜äº‹ã®å†…å®¹ã‚’1-2æ®µè½ã§ç°¡æ½”ã«èª¬æ˜

ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘
â€¢ ãƒã‚¤ãƒ³ãƒˆ1
â€¢ ãƒã‚¤ãƒ³ãƒˆ2
â€¢ ãƒã‚¤ãƒ³ãƒˆ3
ï¼ˆåŒ»ç™‚ãƒ»ç—…é™¢çµŒå–¶ã«é–¢é€£ã™ã‚‹é‡è¦ãªå†…å®¹ã‚’ç®‡æ¡æ›¸ãã§ï¼‰

ã€è©³ç´°è§£èª¬ã€‘
è¨˜äº‹ã®é‡è¦ãªéƒ¨åˆ†ã«ã¤ã„ã¦ã€èƒŒæ™¯ã‚„æ„å‘³ã‚’å«ã‚ã¦è©³ã—ãè§£èª¬
ï¼ˆèª­ã¿ã‚„ã™ã„ã‚ˆã†ã«æ®µè½ã‚’åˆ†ã‘ã¦è¨˜è¼‰ï¼‰

ã€å®Ÿå‹™ã¸ã®å½±éŸ¿ã€‘
â€¢ ç—…é™¢é‹å–¶ã¸ã®å½±éŸ¿
â€¢ åŒ»ç™‚å¾“äº‹è€…ãŒæ³¨æ„ã™ã¹ãç‚¹
â€¢ ä»Šå¾Œäºˆæƒ³ã•ã‚Œã‚‹å¤‰åŒ–

ã€ã¾ã¨ã‚ã€‘
è¨˜äº‹å…¨ä½“ã®è¦ç‚¹ã‚’1æ®µè½ã§ã¾ã¨ã‚ã‚‹

æ³¨æ„äº‹é …:
- å°‚é–€ç”¨èªã¯å¿…è¦ã«å¿œã˜ã¦èª¬æ˜ã‚’åŠ ãˆã‚‹
- æ•°å€¤ã‚„æ—¥ä»˜ãªã©ã®å…·ä½“çš„ãªæƒ…å ±ã¯æ­£ç¢ºã«è¨˜è¼‰
- èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«é©åˆ‡ã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
- åŒ»ç™‚é–¢ä¿‚è€…ã«ã¨ã£ã¦å®Ÿç”¨çš„ãªæƒ…å ±ã‚’é‡è¦–
"""
            return gemini.summarize(prompt)

        except Exception as e:
            return f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}"

    def save_digest(self, articles: List[Dict]):
        """è¨˜äº‹æƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆè¨˜äº‹å†…å®¹ã‚’å–å¾—ã—ã¦è¦ç´„ã‚’å«ã‚ã‚‹ï¼‰"""
        today = datetime.date.today()
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¿®æ­£
        md_file = self.output_dir / f"medifax_digest_{today}.md"
        txt_file = self.output_dir / f"medifax_digest_{today}.txt"

        # å†…å®¹ã‚’ä½œæˆ
        content_lines = []
        content_lines.append(f"åŒ»ç™‚æƒ…å ±ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ - {today}")
        content_lines.append("=" * 80)
        content_lines.append(f"å–å¾—æ—¥æ™‚: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        content_lines.append("=" * 80)
        content_lines.append("")

        for i, article in enumerate(articles, 1):
            print(f"\nè¨˜äº‹ {i}/{len(articles)} ã‚’å‡¦ç†ä¸­: {article['title']}")

            # è¨˜äº‹å†…å®¹ã‚’å–å¾—
            content = self.fetch_article_content(article['link'])

            # å†…å®¹ã‚’è¦ç´„
            if content:
                summary = self.summarize_article(article['title'], content)
            else:
                summary = "è¨˜äº‹å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

            # æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›
            content_lines.append(f"ã€è¨˜äº‹ {i}ã€‘")
            content_lines.append("")
            content_lines.append("ã‚¿ã‚¤ãƒˆãƒ«ï¼š")
            content_lines.append(article['title'])
            content_lines.append("")
            content_lines.append("è§£èª¬ã¨è¦ç´„ï¼š")
            content_lines.append(summary)
            content_lines.append("")
            content_lines.append("URLï¼š")
            content_lines.append(article['link'])
            content_lines.append("")
            content_lines.append("-" * 80)
            content_lines.append("")

        # MDãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        with open(md_file, 'w', encoding='utf-8') as f:
            # Markdownç‰ˆ
            f.write(f"# åŒ»ç™‚æƒ…å ±ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ - {today}\n\n")
            f.write(f"å–å¾—æ—¥æ™‚: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("=" * 80 + "\n\n")

            for i, article in enumerate(articles, 1):
                # MDãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æ—¢ã«å–å¾—æ¸ˆã¿ã®è¦ç´„ã‚’ä½¿ç”¨
                idx = content_lines.index(f"ã€è¨˜äº‹ {i}ã€‘")
                title_idx = content_lines.index("ã‚¿ã‚¤ãƒˆãƒ«ï¼š", idx) + 1
                summary_idx = content_lines.index("è§£èª¬ã¨è¦ç´„ï¼š", idx) + 1
                url_idx = content_lines.index("URLï¼š", idx) + 1

                f.write(f"## è¨˜äº‹ {i}\n\n")
                f.write(f"**ã‚¿ã‚¤ãƒˆãƒ«ï¼š**\n{content_lines[title_idx]}\n\n")

                # è¦ç´„éƒ¨åˆ†ã‚’å–å¾—
                summary_end_idx = content_lines.index("URLï¼š", summary_idx)
                summary_text = "\n".join(content_lines[summary_idx:summary_end_idx-1])
                f.write(f"**è§£èª¬ã¨è¦ç´„ï¼š**\n{summary_text}\n\n")
                f.write(f"**URLï¼š**\n{content_lines[url_idx]}\n\n")
                f.write("-" * 80 + "\n\n")

        # TXTãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆTeamsç”¨ã€UTF-8 with BOMï¼‰
        with open(txt_file, 'w', encoding='utf-8-sig') as f:
            f.write("\n".join(content_lines))

        print(f"\nMDãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {md_file}")
        print(f"TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {txt_file}")

        # æœˆåˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ç§»å‹•å‡¦ç†
        self.move_to_monthly_folder(md_file, today)
        self.move_to_monthly_folder(txt_file, today)

        return txt_file  # Teamsé€ä¿¡ç”¨ã«TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™

    def move_to_monthly_folder(self, file_path: Path, date: datetime.date):
        """å‰æœˆä½œæˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿æœˆåˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ã€‚ä»Šæœˆä½œæˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç§»å‹•ã—ãªã„"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥ã‚’å–å¾—
            stat = file_path.stat()
            created_dt = datetime.datetime.fromtimestamp(stat.st_mtime)
            created_date = created_dt.date()
            # ä»Šæœˆã®1æ—¥
            first_day_of_this_month = datetime.date(date.year, date.month, 1)
            # å‰æœˆã®1æ—¥
            if date.month == 1:
                prev_month = 12
                prev_year = date.year - 1
            else:
                prev_month = date.month - 1
                prev_year = date.year
            first_day_of_prev_month = datetime.date(prev_year, prev_month, 1)
            first_day_of_next_month = (first_day_of_this_month + datetime.timedelta(days=32)).replace(day=1)

            # ä½œæˆæ—¥ãŒå‰æœˆã«è©²å½“ã™ã‚‹å ´åˆã®ã¿ç§»å‹•
            if first_day_of_prev_month <= created_date < first_day_of_this_month:
                monthly_folder = self.output_dir / f"{prev_year:04d}-{prev_month:02d}"
                monthly_folder.mkdir(parents=True, exist_ok=True)
                new_file_path = monthly_folder / file_path.name
                file_path.rename(new_file_path)
                print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰æœˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•: {new_file_path}")
            else:
                print("ä»Šæœˆä½œæˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚ç§»å‹•ã—ã¾ã›ã‚“")
        except Exception as e:
            print(f"æœˆåˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ç§»å‹•ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ®‹ã™

    def send_to_teams(self, articles: List[Dict], file_path: Path):
        """Teamsã®ãƒãƒ£ãƒ³ãƒãƒ«ã« @ä¸€èˆ¬ ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä»˜ãã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒªãƒ³ã‚¯ã‚’æŒ¿å…¥"""
        try:
            # teams.pyã®token_cacheã®ä»•çµ„ã¿ã‚’å‚è€ƒã«access_tokenã‚’å–å¾—
            import teams
            # MSALã®PublicClientApplicationã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨
            app = teams.app
            accounts = app.get_accounts()
            result = None
            if accounts:
                # æ—¢å­˜ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã‚µã‚¤ãƒ¬ãƒ³ãƒˆèªè¨¼
                result = app.acquire_token_silent(teams.SCOPES, account=accounts[0])
            if not result:
                print("Teamsèªè¨¼: ã‚µã‚¤ãƒ¬ãƒ³ãƒˆèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§èªè¨¼ã—ã¦ãã ã•ã„ã€‚")
                flow = app.initiate_device_flow(scopes=teams.SCOPES)
                if "user_code" not in flow:
                    raise Exception("ãƒ‡ãƒã‚¤ã‚¹ãƒ•ãƒ­ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                print(flow["message"])
                result = app.acquire_token_by_device_flow(flow)
            if "access_token" not in result:
                raise Exception("ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {}".format(result.get("error_description")))
            access_token = result["access_token"]

            team_id    = os.environ["TEAMS_TARGET_TEAM_ID"]    # å¯¾è±¡Team
            channel_id = os.environ["TEAMS_TARGET_CHANNEL_ID"] # å¯¾è±¡Channelï¼ˆä¸€èˆ¬ ãªã©ï¼‰

            today = datetime.date.today()

            # ------------------------------
            # 1) ãƒãƒ£ãƒ³ãƒãƒ«ã®Filesãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾— â†’ ãã®é…ä¸‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            # ------------------------------
            base = "https://graph.microsoft.com/v1.0"
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json"
            }

            # ãƒãƒ£ãƒ³ãƒãƒ«ã®Filesãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã‚’å–å¾—
            r = requests.get(f"{base}/teams/{team_id}/channels/{channel_id}/filesFolder", headers=headers)
            r.raise_for_status()
            files_folder = r.json()
            drive_id = files_folder["parentReference"]["driveId"]
            parent_item_id = files_folder["id"]

            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå°ï½ä¸­ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«æƒ³å®šã€‚å¤§ãã„å ´åˆã¯UploadSessionæ¨å¥¨ï¼‰
            upload_headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/octet-stream"
            }
            file_name = file_path.name

            with open(file_path, "rb") as f:
                ur = requests.put(
                    f"{base}/drives/{drive_id}/items/{parent_item_id}:/{file_name}:/content",
                    headers=upload_headers,
                    data=f
                )
            ur.raise_for_status()
            uploaded = ur.json()
            file_web_url = uploaded["webUrl"]  # SharePointä¸Šã®Web URL

            # ------------------------------
            # 2) HTMLæœ¬æ–‡ã‚’ç”Ÿæˆï¼ˆ<at id="0">ä¸€èˆ¬</at> + è¨˜äº‹ä¸€è¦§ + ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒ³ã‚¯ï¼‰
            # ------------------------------
            content_parts = []
            content_parts.append(f"<h2>æœ¬æ—¥ã®åŒ»ç™‚æƒ…å ±ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ</h2>")
            content_parts.append("<at id='0'>ä¸€èˆ¬</at><br/>")
            content_parts.append(f"<strong>ğŸ“… æ—¥ä»˜:</strong> {today}<br/>")
            content_parts.append(f"<strong>ğŸ“Š å–å¾—è¨˜äº‹æ•°:</strong> {len(articles)}ä»¶<br/>")
            content_parts.append("<br/>")
            content_parts.append("<strong>ğŸ“° æœ¬æ—¥ã®è¨˜äº‹ä¸€è¦§:</strong><br/>")
            content_parts.append("<br/>")

            for i, article in enumerate(articles, 1):
                title = article.get("title", "(ç„¡é¡Œ)")
                url = article.get("url")
                if url:
                    content_parts.append(f"{i}. <a href=\"{url}\">{title}</a><br/>")
                else:
                    content_parts.append(f"{i}. {title}<br/>")

            content_parts.append("<br/>")
            content_parts.append("<strong>ğŸ“ è©³ç´°ãªè§£èª¬ã¨è¦ç´„ã¯ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚</strong><br/>")
            content_parts.append(f"<a href=\"{file_web_url}\">{file_name}</a><br/>")
            content = "".join(content_parts)

            # ------------------------------
            # 3) mentions é…åˆ—ã‚’ä»˜ã‘ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿
            #    â€» <at id='0'> ã® "0" ã¨ä¸‹è¨˜ mentions[].id ã‚’ä¸€è‡´ã•ã›ã‚‹
            # ------------------------------
            payload = {
                "body": {
                    "contentType": "html",
                    "content": content
                },
                "mentions": [
                    {
                        "id": 0,
                        "mentionText": "ä¸€èˆ¬",
                        "mentioned": {
                            "conversation": {
                                "id": channel_id,
                                "displayName": "ä¸€èˆ¬"
                            }
                        }
                    }
                ]
            }

            mr = requests.post(
                f"{base}/teams/{team_id}/channels/{channel_id}/messages",
                headers=headers,
                json=payload
            )
            mr.raise_for_status()

            print("Teamsã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¾ã—ãŸ")

        except KeyError as ke:
            print(f"ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™: {ke}. å¿…è¦: TEAMS_TARGET_TEAM_ID, TEAMS_TARGET_CHANNEL_ID")
        except requests.HTTPError as he:
            # Graphã‹ã‚‰ã®è©³ç´°ã‚¨ãƒ©ãƒ¼ã‚’å¯è¦–åŒ–
            try:
                err_detail = he.response.json()
            except Exception:
                err_detail = he.response.text
            print(f"Teamsé€ä¿¡ã‚¨ãƒ©ãƒ¼ (HTTP): {he} | è©³ç´°: {err_detail}")
        except Exception as e:
            print(f"Teamsé€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        print("åŒ»ç™‚æƒ…å ±RSSãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ãƒ»è¦ç´„ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"å‡¦ç†é–‹å§‹: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # 1. ãƒ­ã‚°ã‚¤ãƒ³è¨­å®š
        login_success = self.setup_safari_automation()
        if not login_success:
            print("ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # 2. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆæœ¬æ—¥åˆ†ã®ã¿ï¼‰
        articles = self.fetch_rss_feed()
        if not articles:
            print("æœ¬æ—¥åˆ†ã®è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return

        # 3. çµæœã‚’ä¿å­˜ï¼ˆè¦ç´„ãƒ»jsonã¯å‡ºåŠ›ã—ãªã„ï¼‰
        txt_file = self.save_digest(articles)

        # 4. Teamsã«é€ä¿¡
        self.send_to_teams(articles, txt_file)

        # 5. TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆTeamsé€ä¿¡å¾Œï¼‰
        try:
            if txt_file.exists():
                txt_file.unlink()
                print(f"TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {txt_file}")
        except Exception as e:
            print(f"TXTãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

        print(f"\nå‡¦ç†å®Œäº†: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å‡¦ç†ã—ãŸè¨˜äº‹æ•°: {len(articles)}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    digest = MedifaxAutoLogin()
    digest.run()


if __name__ == "__main__":
    main()